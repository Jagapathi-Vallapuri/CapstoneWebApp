# Example environment variables for backend
# Generate a strong SECRET_KEY (keep private)
SECRET_KEY=replace_me_with_a_secure_random_string
DATABASE_URL=mysql+pymysql://user:password@localhost:3306/capstone_db

# JWT settings (optional)
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# AWS S3 settings (optional if not using S3)
S3_BUCKET=
S3_REGION=
S3_ACCESS_KEY_ID=
S3_SECRET_ACCESS_KEY=

# Comma-separated list of allowed origins for CORS (e.g. http://localhost:3000,https://example.com)
ALLOWED_ORIGINS=http://localhost:5173

# LLM / Chat settings
# Provider name: 'lmstudio' or 'openai'
LLM_PROVIDER=lmstudio
# LM Studio generate endpoint (example for a local LM Studio instance)
# e.g. http://127.0.0.1:8080/api/generate
LLM_API_URL=
# API key / token if your LM Studio instance requires one (leave empty if none)
LLM_API_KEY=
# Optional model identifier (depends on provider)
LLM_MODEL=
# Tuning parameters
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.2

# --- Gemini (Google Generative Language API) example ---
# To use Gemini instead of LM Studio, set provider to 'gemini' and supply the API key.
# LLM_PROVIDER=gemini
# LLM_API_KEY=your-gemini-api-key
# Optional model (defaults to gemini-2.5-flash if omitted)
# LLM_MODEL=gemini-2.5-flash
# Leave LLM_API_URL empty when using Gemini; the backend constructs the endpoint automatically.
